[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAnalysis of pizza restaurants\n\n\nA TidyTuesday exercise\n\n\n\nR\n\n\nData Analysis\n\n\nTidy Tuesday\n\n\n\nAn analysis of TidyTuesday data for pizza restaurants and their ratings.\n\n\n\n\n\nOct 12, 2019\n\n\nAndreas Handel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Winnie Shi",
    "section": "",
    "text": "Hey there! I‚Äôm Winnie, a college student who loves camping and cooking. I focus on data science and trans and queer studies, and I‚Äôm always excited to connect with new people. Thanks for stopping by!\n\n\nMacalester College | St.¬†Paul MN  B.A. in Data Science, Women Gender and Sexuality Studies | Sept 2024 - May 2026"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Winnie Shi",
    "section": "",
    "text": "Macalester College | St.¬†Paul MN  B.A. in Data Science, Women Gender and Sexuality Studies | Sept 2024 - May 2026"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Analysis of pizza restaurants",
    "section": "",
    "text": "This analysis was performed as part of an exercise for my Modern Applied Data Analysis course.\nWhen I taught the course in fall 2019, one of the weekly assignments for the students was to participate in TidyTuesday. I did the exercise as well, this is my product. You can get the R Markdown/Quarto file to re-run the analysis here.\n\nIntroduction\nIf you are not familiar with TidyTuesday, you can take a quick look at the TidyTuesday section on this page.\nThis week‚Äôs data was all about Pizza. More on the data is here.\n\n\nLoading packages\n\nlibrary('readr')\nlibrary('ggplot2')\nlibrary(\"dplyr\")\nlibrary(\"cowplot\")\nlibrary(\"plotly\")\nlibrary(\"forcats\")\nlibrary(\"geosphere\")\nlibrary(\"emoji\")\n\n\n\nData loading\nLoad date following TidyTueday instructions.\n\npizza_jared &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_jared.csv\")\npizza_barstool &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_barstool.csv\")\npizza_datafiniti &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_datafiniti.csv\")\n\n\n\nAnalysis Ideas\nSee the TidyTuesday website for a codebook. These are 3 datasets. Looks like the 1st dataset is ratings of pizza places through some (online?) survey/poll, the 2nd dataset again has ratings of pizza places from various sources, and the 3rd dataset seems to have fairly overlapping information to the 2nd dataset.\nNote: When I looked at the website, the codebook for the 3rd dataset seemed mislabeled. Might be fixed by now.\nPossibly interesting questions I can think of:\n\nFor a given pizza restaurant, how do the different ratings/scores agree or differ?\nAre more expensive restaurants overall rated higher?\nIs there some systematic dependence of rating on location? Do restaurants located in a certain area in general get rated higher/lower compared to others?\n\nI think those are good enough questions to figure out, let‚Äôs see how far we get.\n\n\nInitial data exploration\nStart with a quick renaming and general check.\n\n#saves typing\nd1 &lt;- pizza_jared \nd2 &lt;- pizza_barstool \nd3 &lt;- pizza_datafiniti \nglimpse(d1)\n\nRows: 375\nColumns: 9\n$ polla_qid   &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5‚Ä¶\n$ answer      &lt;chr&gt; \"Excellent\", \"Good\", \"Average\", \"Poor\", \"Never Again\", \"Ex‚Ä¶\n$ votes       &lt;dbl&gt; 0, 6, 4, 1, 2, 1, 1, 3, 1, 1, 4, 2, 1, 1, 0, 1, 1, 0, 3, 0‚Ä¶\n$ pollq_id    &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5‚Ä¶\n$ question    &lt;chr&gt; \"How was Pizza Mercato?\", \"How was Pizza Mercato?\", \"How w‚Ä¶\n$ place       &lt;chr&gt; \"Pizza Mercato\", \"Pizza Mercato\", \"Pizza Mercato\", \"Pizza ‚Ä¶\n$ time        &lt;dbl&gt; 1344361527, 1344361527, 1344361527, 1344361527, 1344361527‚Ä¶\n$ total_votes &lt;dbl&gt; 13, 13, 13, 13, 13, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 5, 5, 5,‚Ä¶\n$ percent     &lt;dbl&gt; 0.0000, 0.4615, 0.3077, 0.0769, 0.1538, 0.1429, 0.1429, 0.‚Ä¶\n\nglimpse(d2)\n\nRows: 463\nColumns: 22\n$ name                                 &lt;chr&gt; \"Pugsley's Pizza\", \"Williamsburg ‚Ä¶\n$ address1                             &lt;chr&gt; \"590 E 191st St\", \"265 Union Ave\"‚Ä¶\n$ city                                 &lt;chr&gt; \"Bronx\", \"Brooklyn\", \"New York\", ‚Ä¶\n$ zip                                  &lt;dbl&gt; 10458, 11211, 10017, 10036, 10003‚Ä¶\n$ country                              &lt;chr&gt; \"US\", \"US\", \"US\", \"US\", \"US\", \"US‚Ä¶\n$ latitude                             &lt;dbl&gt; 40.85877, 40.70808, 40.75370, 40.‚Ä¶\n$ longitude                            &lt;dbl&gt; -73.88484, -73.95090, -73.97411, ‚Ä¶\n$ price_level                          &lt;dbl&gt; 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, ‚Ä¶\n$ provider_rating                      &lt;dbl&gt; 4.5, 3.0, 4.0, 4.0, 3.0, 3.5, 3.0‚Ä¶\n$ provider_review_count                &lt;dbl&gt; 121, 281, 118, 1055, 143, 28, 95,‚Ä¶\n$ review_stats_all_average_score       &lt;dbl&gt; 8.011111, 7.774074, 5.666667, 5.6‚Ä¶\n$ review_stats_all_count               &lt;dbl&gt; 27, 27, 9, 2, 1, 4, 5, 17, 14, 6,‚Ä¶\n$ review_stats_all_total_score         &lt;dbl&gt; 216.3, 209.9, 51.0, 11.2, 7.1, 16‚Ä¶\n$ review_stats_community_average_score &lt;dbl&gt; 7.992000, 7.742308, 5.762500, 0.0‚Ä¶\n$ review_stats_community_count         &lt;dbl&gt; 25, 26, 8, 0, 0, 3, 4, 16, 13, 4,‚Ä¶\n$ review_stats_community_total_score   &lt;dbl&gt; 199.8, 201.3, 46.1, 0.0, 0.0, 13.‚Ä¶\n$ review_stats_critic_average_score    &lt;dbl&gt; 8.8, 0.0, 0.0, 4.3, 0.0, 0.0, 0.0‚Ä¶\n$ review_stats_critic_count            &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ‚Ä¶\n$ review_stats_critic_total_score      &lt;dbl&gt; 8.8, 0.0, 0.0, 4.3, 0.0, 0.0, 0.0‚Ä¶\n$ review_stats_dave_average_score      &lt;dbl&gt; 7.7, 8.6, 4.9, 6.9, 7.1, 3.2, 6.1‚Ä¶\n$ review_stats_dave_count              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ review_stats_dave_total_score        &lt;dbl&gt; 7.7, 8.6, 4.9, 6.9, 7.1, 3.2, 6.1‚Ä¶\n\nglimpse(d3)\n\nRows: 10,000\nColumns: 10\n$ name            &lt;chr&gt; \"Shotgun Dans Pizza\", \"Sauce Pizza Wine\", \"Mios Pizzer‚Ä¶\n$ address         &lt;chr&gt; \"4203 E Kiehl Ave\", \"25 E Camelback Rd\", \"3703 Paxton ‚Ä¶\n$ city            &lt;chr&gt; \"Sherwood\", \"Phoenix\", \"Cincinnati\", \"Madison Heights\"‚Ä¶\n$ country         &lt;chr&gt; \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", ‚Ä¶\n$ province        &lt;chr&gt; \"AR\", \"AZ\", \"OH\", \"MI\", \"MD\", \"MD\", \"CA\", \"CA\", \"FL\", ‚Ä¶\n$ latitude        &lt;dbl&gt; 34.83230, 33.50927, 39.14488, 42.51667, 39.28663, 39.2‚Ä¶\n$ longitude       &lt;dbl&gt; -92.18380, -112.07304, -84.43269, -83.10663, -76.56698‚Ä¶\n$ categories      &lt;chr&gt; \"Pizza,Restaurant,American restaurants,Pizza Place,Res‚Ä¶\n$ price_range_min &lt;dbl&gt; 0, 0, 0, 25, 0, 0, 0, 0, 0, 0, 25, 25, 25, 25, 0, 0, 0‚Ä¶\n$ price_range_max &lt;dbl&gt; 25, 25, 25, 40, 25, 25, 25, 25, 25, 25, 40, 40, 40, 40‚Ä¶\n\n\nThe first question I have is if the pizza places in the 3 datasets are the same or at least if there is decent overlap. If not, then one can‚Äôt combine the data.\n\nd1names = unique(d1$place)\nd2names = unique(d2$name)\nd3names = unique(d3$name)\nsum(d1names %in% d2names) #check how many restaurants in d1 are also in d2. Note that this assumes exact spelling.\n\n[1] 22\n\nsum(d1names %in% d3names) #check how many restaurants in d1 are also in d2. Note that this assumes exact spelling.\n\n[1] 9\n\nsum(d2names %in% d3names)\n\n[1] 66\n\n\n22 restaurants out of 56 in dataset 1 are also in dataset 2. Only 9 overlap between dataset 1 and 3. 66 are shared between datasets 2 and 3.\nThe last dataset has no ratings, and if I look at the overlap of dataset 1 and 2, I only get a few observations. So I think for now I‚Äôll focus on dataset 2 and see if I can address the 3 questions I posed above with just that dataset. Maybe I‚Äôll have ideas for the other 2 datasets as I go along (would be a shame to not use them.)\n\n\nRatings agreement analysis\nOk, I‚Äôll focus on dataset 2 now and look closer at the scores/rating. From the codebook, it‚Äôs not quite clear to me what the different scores and counts in dataset 2 actually mean, so let‚Äôs look closer to try and figure that out.\nFrom the glimpse function above, I can‚Äôt see much of a difference between average and total score. Let‚Äôs look at that. Here are a few plots comparing the different score-related variables.\n\nplot(d2$review_stats_community_total_score,d2$review_stats_community_average_score)\n\n\n\n\n\n\n\nplot(d2$review_stats_community_total_score - d2$review_stats_community_average_score* d2$review_stats_community_count)\n\n\n\n\n\n\n\nplot(d2$review_stats_critic_total_score-d2$review_stats_critic_average_score)\n\n\n\n\n\n\n\nplot(d2$review_stats_dave_total_score-d2$review_stats_dave_average_score)\n\n\n\n\n\n\n\nplot(d2$review_stats_all_total_score- (d2$review_stats_community_total_score+d2$review_stats_critic_total_score+d2$review_stats_dave_total_score))  \n\n\n\n\n\n\n\n\nOk, so based on the plots above, and a few other things I tried, it seems that average score is total score divided by number of counts, and the all score is just the sum of dave, critic and community.\nSo to address my first question, I‚Äôll look at correlations between average scores for the 3 types of reviewers, namely dave, critic and community.\nHowever, while playing around with the data in the last section, I noticed a problem. Look at the counts for say critics and the average score.\n\ntable(d2$review_stats_critic_count)\n\n\n  0   1   5 \n401  61   1 \n\ntable(d2$review_stats_critic_average_score)\n\n\n   0    4  4.3  4.5  4.8    5  5.1  5.4  5.5  5.7  5.8  5.9 5.96    6  6.2 6.31 \n 401    1    1    1    1    3    1    1    1    1    1    1    1    1    1    1 \n 6.5  6.6  6.7 6.76  6.8  6.9    7  7.2  7.3  7.4  7.6 7.76  7.8  7.9    8  8.1 \n   3    1    1    1    2    1    5    2    2    1    1    1    2    1    4    2 \n 8.5  8.7  8.8    9  9.3  9.4  9.8   10   11 \n   3    1    1    1    1    2    1    4    1 \n\n\nA lot of restaurants did not get reviewed by critics, and the score is coded as 0. That‚Äôs a problem since if we take averages and such, it will mess up things. This should really be counted as NA. So let‚Äôs create new average scores such that any restaurant with no visits/reviews gets an NA as score.\n\nd2 &lt;- d2 %&gt;% mutate( comm_score = ifelse(review_stats_community_count == 0 ,NA,review_stats_community_average_score)) %&gt;%\n             mutate( crit_score = ifelse(review_stats_critic_count == 0 ,NA,review_stats_critic_average_score)) %&gt;%\n             mutate( dave_score = ifelse(review_stats_dave_count == 0 ,NA,review_stats_dave_average_score)) \n\nNow let‚Äôs plot the 3.\n\np1 &lt;- d2 %&gt;% ggplot(aes(x=comm_score, y = crit_score)) + geom_point() + geom_smooth(method = \"lm\")\np2 &lt;- d2 %&gt;% ggplot(aes(x=comm_score, y = dave_score)) + geom_point() + geom_smooth(method = \"lm\")\np3 &lt;- d2 %&gt;% ggplot(aes(x=crit_score, y = dave_score)) + geom_point() + geom_smooth(method = \"lm\")\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLooks like there is some agreement between Dave, the critics and the community on the ratings of various pizza places, though there is a good bit of variation.\nI think it would be fun to be able to click on specific points to see for a given score which restaurant that is. For instance I‚Äôm curious which restaurant has a close to zero score from both the community and Dave (bottom left of plot B).\nI think that can be done with plotly, let‚Äôs google it.\nOk, figured it out. This re-creates the 3 scatterplots from above and when one moves over the dots, it shows restaurant name.\n\nplotly::plot_ly(d2, x = ~comm_score, y = ~crit_score, type = \"scatter\", mode = 'markers', text = ~paste('Restaurant: ', d2$name))\n\n\n\n\nplotly::plot_ly(d2, x = ~comm_score, y = ~dave_score, type = \"scatter\", mode = 'markers', text = ~paste('Restaurant: ', d2$name))\n\n\n\n\nplotly::plot_ly(d2, x = ~crit_score, y = ~dave_score, type = \"scatter\", mode = 'markers', text = ~paste('Restaurant: ', d2$name))\n\n\n\n\n\nSo apparently the lousy restaurant that got a 1 from the community and almost 0 from Dave is called Amtrak. I‚Äôm wondering if that refers to pizza on Amtrak trains? Just for the heck of it and because I‚Äôm curious, let‚Äôs look at that entry.\n\nd2 %&gt;% filter(name == \"Amtrak\") %&gt;% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\naddress1\ncity\nzip\ncountry\nlatitude\nlongitude\nprice_level\nprovider_rating\nprovider_review_count\nreview_stats_all_average_score\nreview_stats_all_count\nreview_stats_all_total_score\nreview_stats_community_average_score\nreview_stats_community_count\nreview_stats_community_total_score\nreview_stats_critic_average_score\nreview_stats_critic_count\nreview_stats_critic_total_score\nreview_stats_dave_average_score\nreview_stats_dave_count\nreview_stats_dave_total_score\ncomm_score\ncrit_score\ndave_score\n\n\n\n\nAmtrak\n234 W 31st St\nNew York\n10001\nUS\n40.74965\n-73.9934\n0\n3\n345\n0.54\n2\n1.08\n1\n1\n1\n0\n0\n0\n0.08\n1\n0.08\n1\nNA\n0.08\n\n\n\n\n\nI googled the address, and it seems to be indeed Amtrak. Note to self: Never order pizza on an Amtrak train.\n\n\nPrice vs ratings analysis\nNext, let‚Äôs look at possible impact of restaurant price level on rating.\n\ntable(d2$price_level)\n\n\n  0   1   2   3 \n 21 216 218   8 \n\n\nThere isn‚Äôt much spread, most pizza places are in the middle. Maybe not too surprising. Let‚Äôs look at a few plots to see if there is a pattern. First, we should recode price level as a factor.\n\nd2 &lt;- d2 %&gt;% mutate(price = as.factor(price_level))\n\n\np1 &lt;- d2 %&gt;% ggplot(aes(x=price, y=comm_score)) + geom_violin() + geom_point()\np2 &lt;- d2 %&gt;% ggplot(aes(x=price, y=crit_score)) + geom_violin() + geom_point()\np3 &lt;- d2 %&gt;% ggplot(aes(x=price, y=dave_score)) + geom_violin() + geom_point()\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n\n\n\n\n\n\n\nHard to tell if there‚Äôs a trend. Could do some stats to look in more detail, but since this exercise focuses on exploring, I won‚Äôt do that. Instead I‚Äôll leave it at that.\n\n\nRating versus location\nOk, on to the last of the questions I started out with. Maybe there are some areas where restaurants are in general better? Or maybe an area where diners are more critical? Let‚Äôs see if there is some correlation between ratings and location.\n\ntable(d2$country)\n\n\n US \n463 \n\nsort(table(d2$city))\n\n\n        Alpharetta            Augusta             Austin         Austintown \n                 1                  1                  1                  1 \n        Blacksburg          Braintree           Brockton            Buffalo \n                 1                  1                  1                  1 \n        Charleston          Charlotte      Chestnut Hill           Chilmark \n                 1                  1                  1                  1 \n        Clearwater            Clifton         Coralville      Daytona Beach \n                 1                  1                  1                  1 \n          Dearborn        Dennis Port              DUMBO        East Meadow \n                 1                  1                  1                  1 \n             Edina          Elizabeth             Elmont         Gansevoort \n                 1                  1                  1                  1 \n              Gary       Hampton Bays          Hopkinton       Howard Beach \n                 1                  1                  1                  1 \n        Huntington          Iowa City            Jackson Jacksonville Beach \n                 1                  1                  1                  1 \n       Jersey City        Kew Gardens          Lakeville      Lawrenceville \n                 1                  1                  1                  1 \n              Lynn    Manhattan Beach       Mashantucket              Miami \n                 1                  1                  1                  1 \n    Middle Village       Mount Vernon      New Hyde Park      New York City \n                 1                  1                  1                  1 \n   North Arlington             Nutley         Oak Bluffs           Oak Lawn \n                 1                  1                  1                  1 \n     Oklahoma City             Orange         Palm Beach           Pembroke \n                 1                  1                  1                  1 \n         Princeton             Ramsey           Randolph             Revere \n                 1                  1                  1                  1 \n        Rutherford      San Francisco           Sandwich        Southampton \n                 1                  1                  1                  1 \n         Stoughton              Tampa     Vineyard Haven     West Melbourne \n                 1                  1                  1                  1 \n   West Palm Beach       West Roxbury             Woburn            Yonkers \n                 1                  1                  1                  1 \n   East Rutherford          Edgartown       Elmwood Park            Hyannis \n                 2                  2                  2                  2 \n       Miami Beach       Philadelphia         Saint Paul       Santa Monica \n                 2                  2                  2                  2 \n          Stamford              Bronx       Indianapolis          Lexington \n                 2                  3                  3                  3 \n        Morgantown        San Antonio          San Diego          Ann Arbor \n                 3                  3                  3                  4 \n        Louisville          New Haven      Staten Island         Youngstown \n                 4                  4                  4                  4 \n           Atlanta            Chicago           Columbus            Hoboken \n                 6                  6                  6                  6 \n         Nantucket   Saratoga Springs        Minneapolis          Las Vegas \n                 6                  6                  8                 11 \n            Boston           Brooklyn           New York \n                13                 20                251 \n\n\nOk so all restaurants are in the US, and most are in New York. We could look at NY versus ‚Äúrest of the cities‚Äù. Though isn‚Äôt Brooklyn (the 2nd largest entry) basically a part of New York? I‚Äôm not enough of an expert on all things NY to be sure (for any real analysis, you need to know a good bit about the subject matter, or work closely with a subject matter expert. If not, more likely than not something dumb will happen).\nFor now, I assume that it‚Äôs different enough, and make 2 categories, NY and ‚Äúother‚Äù and see if there are differences. Let‚Äôs try.\n\np1 &lt;- d2 %&gt;% dplyr::mutate(newcity = forcats::fct_lump(city, n = 1)) %&gt;%\n              ggplot(aes(x=newcity, y = comm_score)) + geom_violin() + geom_point()\np2 &lt;- d2 %&gt;% dplyr::mutate(newcity = forcats::fct_lump(city, n = 1)) %&gt;%\n              ggplot(aes(x=newcity, y = crit_score)) + geom_violin() + geom_point()\np3 &lt;- d2 %&gt;% dplyr::mutate(newcity = forcats::fct_lump(city, n = 1)) %&gt;%\n              ggplot(aes(x=newcity, y = dave_score)) + geom_violin() + geom_point()\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n\n\n\n\n\n\n\nLooks like the community in NY gives lower scores compared to other locations, less noticeable difference for critics and Dave.\nOk, the next analysis might not make much sense, but why not check if there is a North-South or East-West trend related to ratings. Maybe restaurants are better in one of those directions? Or people in the South are more polite and give better scores? üòÅ. I‚Äôm mostly doing this because longitude and latitude are continuous variables, so I can make a few more scatterplots. I don‚Äôt have any real goal for this otherwise.\n\np1 &lt;- d2 %&gt;%  ggplot(aes(x=longitude, y = comm_score)) + geom_point() + geom_smooth(method = 'lm')\np2 &lt;- d2 %&gt;%  ggplot(aes(x=longitude, y = crit_score)) + geom_point() + geom_smooth(method = 'lm')\np3 &lt;- d2 %&gt;%  ggplot(aes(x=longitude, y = dave_score)) + geom_point() + geom_smooth(method = 'lm')\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nSo as we go from the west (-120) to the east (-70), there is a trend in restaurants getting higher scores, by all 3 groups. I guess as we are moving closer to Italy, the pizza quality goes up? üòÉ.\nNext, let‚Äôs look at latitude.\n\np1 &lt;- d2 %&gt;%  ggplot(aes(x=latitude, y = comm_score)) + geom_point() + geom_smooth(method = 'lm')\np2 &lt;- d2 %&gt;%  ggplot(aes(x=latitude, y = crit_score)) + geom_point() + geom_smooth(method = 'lm')\np3 &lt;- d2 %&gt;%  ggplot(aes(x=latitude, y = dave_score)) + geom_point() + geom_smooth(method = 'lm')\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nSo doesn‚Äôt seem as much of a trend going from South (25) to North (45). That finding of course fully confirms our ‚Äúcloser to Italy‚Äù theory!\nOk, I was going to leave it at that with location, but since I‚Äôm already going down a crazy rabbit hole regarding Italy, let‚Äôs do it for real: We‚Äôll take both longitude and latitude of each restaurant and use it compute the distance of each location to Naples, the home of Pizza. And then we‚Äôll plot that and see.\nSince I have no idea how to do that, I need Google. Fortunately, the first hit worked, found this one: https://stackoverflow.com/questions/32363998/function-to-calculate-geospatial-distance-between-two-points-lat-long-using-r\nLet‚Äôs try.\n\ncoord_naples=cbind(rep(14.2,nrow(d2)),rep(40.8,nrow(d2)))  #location of naples\ncoord_restaurants = cbind(d2$longitude,d2$latitude)\ndistvec = rep(0,nrow(d2))\nfor (n in 1:nrow(d2))\n{\n  distvec[n] = distm( coord_restaurants[n,], coord_naples[n,], fun = distGeo)\n}\nd2$distvec = distvec / 1609 #convert to miles since we are in the US :)\n\nIt‚Äôs not tidyverse style, which I tried first but couldn‚Äôt get it to work. The trusty old for-loop seems to always work for me. I checked the numbers in distvec, they look reasonable.\nOk, let‚Äôs redo the plots above, now with distance to Naples.\n\np1 &lt;- d2 %&gt;%  ggplot(aes(x=distvec, y = comm_score)) + geom_point() + geom_smooth(method = 'lm')\np2 &lt;- d2 %&gt;%  ggplot(aes(x=distvec, y = crit_score)) + geom_point() + geom_smooth(method = 'lm')\np3 &lt;- d2 %&gt;%  ggplot(aes(x=distvec, y = dave_score)) + geom_point() + geom_smooth(method = 'lm')\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12, nrow = 3)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHm ok, no smoking gun. Looks like there is a bit of a trend that the further away you are from Naples, the lower the score. But really not much.\n\n\nHyping our result\nBut since this distance-from-Naples makes such a good story, let‚Äôs see if I can hype it.\nFirst, to increase potential statistical strength, I‚Äôll combine all 3 scores into an overall mean, i.e.¬†similar ot the all variable in the original. I don‚Äôt trust that one since I don‚Äôt know if they averaged over 0 instead of properly treating it as NA. Of course I could check, but I‚Äôm just re-creating it here.\n\nd2$all_score = rowMeans(cbind(d2$dave_score,d2$crit_score,d2$comm_score),na.rm=TRUE)\n\nOk, let‚Äôs check if correlation between this new score and distance is significant!\n\n#compute a linear fit and p-value (it's significant!)\nfit=lm(d2$all_score ~ d2$distvec, data = d2)\nsummary(fit)\n\n\nCall:\nlm(formula = d2$all_score ~ d2$distvec, data = d2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7854 -0.5866  0.3027  0.9612  2.3686 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.9895014  0.7008802  12.826  &lt; 2e-16 ***\nd2$distvec  -0.0004772  0.0001525  -3.129  0.00187 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.478 on 459 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.02089,   Adjusted R-squared:  0.01875 \nF-statistic: 9.791 on 1 and 459 DF,  p-value: 0.001865\n\npval=anova(fit)$`Pr(&gt;F)`[1]\nprint(pval)\n\n[1] 0.001865357\n\n\nIt is signficant, p&lt;0.05! We hit pay dirt! Let‚Äôs make a great looking figure and go tell the press!\n\n#make final plot\np1 &lt;- d2 %&gt;%  ggplot(aes(x=distvec, y = all_score)) + geom_point(shape = 21, colour = \"black\", fill = \"red\",  size = 2 ) + geom_smooth(method = 'lm', se = TRUE, color = \"darkgreen\", size = 2) + xlab('Distance from Naples (miles)') + ylab('Pizza Quality (score)') + ylim(c(2.5,max(d2$all_score))) + theme_bw() +theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=\"bold\")) + annotate(\"text\", x=6000, y=9, label= paste(\"p =\",round(pval,4)),size = 12) \nggsave('pizzadistance.png')\nknitr::include_graphics(\"pizzadistance.png\")\n\n\n\n\n\n\n\n\n\n\nThe ‚Äúpress release‚Äù\nA novel study of pizza restaurants in the US found a clear, statistically significant correlation between the distance of the restaurant to Naples and the quality of the pizza as determined by the community and expert restaurant critics. The study authors attribute the finding to the ability of restaurants that are closer to Naples to more easily get genuine fresh and high quality ingredients, such as the famous San Marzano tomatoes.\n\n\n\n\n\n\n\n\n\n\n\nSummary\nThat was a fun exploration. It was the first time I played with the tidyverse data. I had no idea which direction it was going to go, and ideas just came as I was doing it. I‚Äôm sure there is interesting stuff in datasets 1 and 3 as well, but I already spent several hours on this and will therefore call it quits now.\nWhile the exercise was supposed to focus on cleaning/wrangling and visualizing, I couldn‚Äôt resist going all the way at the end and producing a statistically significant and somewhat plausible sounding finding. If this were a ‚Äúreal‚Äù study/analysis, such a nice result would be happily accepted by most analysts/authors, hyped by a university press release and - if the result is somewhat interesting/cute, picked up by various media outlets.\nI had no idea at the beginning what I was going to analyze, I did that longitude/latitude analysis on a whim, and if I hadn‚Äôt found this correlation and had that crazy distance to Italy idea, nothing would have happened. But now that I have a significant result and a good story to go with, I can publish! It‚Äôs not really much sillier than for instance the Chocolate and Nobel Laureates paper paper.\nWhat I illustrated here (without having had any plan to do so), is a big, general problem in secondary data analysis. It‚Äôs perfectly ok to do secondary analyses, and computing significance is also (kinda) ok, but selling exploratory (fishing) results as inferential/causal/confirmatory is wrong - and incredibly widespread. If you want to sharpen your critical thinking skills related to all those supposed significant and real findings in science we see a lot, a great (though at times sobering) read is Andrew Gelman‚Äôs blog where he regularly picks apart studies/results like the one I did here or the chocolate and Nobel laureates one. And now I‚Äôll go eat some chocolate so I can increase my chances for a Nobel prize."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]